from pyspark.sql.functions import udf
from pyspark.sql.types import DoubleType

# Função auxiliar para cálculo da derivada conforme as regras (passos 5, 6 e 7)
def calc_derivative(x, y, idx, side):
    n = len(x)
    if side == 'left':  # Primeiro ponto (x0)
        if n > 2:
            cond = ((x[2] - x[1]) / (y[2] - y[1])) - ((x[1] - x[0]) / (y[1] - y[0]))
            if cond < 0:
                return (y[1] - y[0]) / (x[1] - x[0])
            else:
                fprime_next = (y[2] - y[1]) / (x[2] - x[1])
                return (3 * (y[1] - y[0]) / (2 * (x[1] - x[0]))) - (fprime_next / 2)
        else:
            return (y[1] - y[0]) / (x[1] - x[0])
    elif side == 'right':  # Último ponto
        if n > 2:
            cond = ((x[n-1] - x[n-2]) / (y[n-1] - y[n-2])) - ((x[n-2] - x[n-3]) / (y[n-2] - y[n-3]))
            if cond < 0:
                return (y[n-1] - y[n-2]) / (x[n-1] - x[n-2])
            else:
                fprime_temp = (y[n-1] - y[n-2]) / (x[n-1] - x[n-2])
                return (3 * (y[n-1] - y[n-2]) / (2 * (x[n-1] - x[n-2]))) - (fprime_temp / 2)
        else:
            return (y[n-1] - y[n-2]) / (x[n-1] - x[n-2])
    else:  # 'interior'
        if idx + 1 < n - 1:
            cond = ((x[idx+2] - x[idx+1]) / (y[idx+2] - y[idx+1])) - ((x[idx+1] - x[idx]) / (y[idx+1] - y[idx]))
            if cond < 0:
                return (y[idx+1] - y[idx-1]) / (x[idx+1] - x[idx-1])
            else:
                fprime_next = (y[idx+2] - y[idx+1]) / (x[idx+2] - x[idx+1])
                return (3 * (y[idx+1] - y[idx]) / (2 * (x[idx+1] - x[idx]))) - (fprime_next / 2)
        else:
            return (y[idx] - y[idx-1]) / (x[idx] - x[idx-1])

# Função para interpolação cúbica constrained unidimensional (Equações 14 a 20)
def cubic_spline_interp(x, y, x_interp):
    n = len(x)
    if x_interp <= x[0]:
        return y[0]
    if x_interp >= x[-1]:
        return y[-1]
    # Localiza o intervalo [x[i-1], x[i]]
    i = 1
    while i < n and x_interp > x[i]:
        i += 1
    x0, x1 = x[i-1], x[i]
    y0, y1 = y[i-1], y[i]
    
    # Calcula as derivadas para os extremos do intervalo
    m0 = calc_derivative(x, y, 0 if i-1==0 else i-1, 'left' if i-1==0 else 'interior')
    m1 = calc_derivative(x, y, n-1 if i==n-1 else i, 'right' if i==n-1 else 'interior')
    
    # Cálculo das segundas derivadas (Equações 14 e 15)
    f2_x0 = (2 * (m1 - m0)) / (x1 - x0) + (6 * (y1 - y0)) / ((x1 - x0)**2)
    f2_x1 = (2 * (m1 + m0)) / (x1 - x0) + (6 * (y1 - y0)) / ((x1 - x0)**2)
    
    # Cálculo dos coeficientes (Equações 16 a 19)
    d = (f2_x1 - f2_x0) / (6 * (x1 - x0))
    c = (x1 * f2_x0 - x0 * f2_x1) / (2 * (x1 - x0))
    b = ((y1 - y0) - c * (x1**2 - x0**2) - d * (x1**3 - x0**3)) / (x1 - x0)
    a = y0 - b * x0 - c * (x0**2) - d * (x0**3)
    
    # Avaliação da spline (Equação 20)
    return a + b * x_interp + c * (x_interp**2) + d * (x_interp**3)

# Interpolação bidimensional: primeiro interpola no eixo delta para cada tempo e depois no eixo tempo
def bidimensional_interp(deltas, tempos, vol_matrix, delta_interp, tempo_interp):
    interp_por_tempo = []
    for row in vol_matrix:
        interp_por_tempo.append(cubic_spline_interp(deltas, row, delta_interp))
    return cubic_spline_interp(tempos, interp_por_tempo, tempo_interp)

# UDF – recebe (volatilidades, tempos, deltas, delta_interp, tempo_interp)
# Assume que:
#   • volatilidades: array de float de tamanho len(tempos)*len(deltas) (ordenado por tempo e, dentro de cada tempo, por delta)
#   • tempos e deltas: arrays de float já ordenados
#   • delta_interp e tempo_interp: floats
@udf(DoubleType())
def interp_vol_udf(volatilidades, tempos, deltas, delta_interp, tempo_interp):
    # Determina quantos deltas há (o grid é pequeno, geralmente)
    n_d = len(deltas)
    n_t = len(tempos)
    # Reconstroi a "matriz" de volatilidades a partir da lista linear:
    # para cada tempo, pega os n_d valores consecutivos
    vol_matrix = [volatilidades[i * n_d : (i + 1) * n_d] for i in range(n_t)]
    return float(bidimensional_interp(deltas, tempos, vol_matrix, delta_interp, tempo_interp))


# Exemplo: DataFrame com uma linha onde:
#   • volatilidades: [vol1, vol2, ..., volN] – tamanho = len(tempos)*len(deltas)
#   • tempos: [t1, t2, ...] (ordenados)
#   • deltas: [d1, d2, ...] (ordenados)
#   • delta_interp e tempo_interp: os floats de consulta

data = [
    (
        [10.0, 20.0, 30.0, 40.0,  # para tempo 1.0 (4 deltas)
         11.0, 21.0, 31.0, 41.0], # para tempo 2.0
        [1.0, 2.0],
        [0.1, 0.2, 0.3, 0.4],
        0.25,  # delta de consulta
        1.5    # tempo de consulta
    )
]
columns = ["volatilidades", "tempos", "deltas", "delta_interp", "tempo_interp"]

df = spark.createDataFrame(data, schema=columns)
df = df.withColumn("vol_interpolated", interp_vol_udf("volatilidades", "tempos", "deltas", "delta_interp", "tempo_interp"))
df.show(truncate=False)

