from pyspark.sql.functions import pandas_udtf
import pandas as pd
from pyspark.sql.types import StructType, StructField, StringType, DoubleType
from typing import Iterator

# --- Funções de Interpolação Unidimensional e Bidimensional ---
# (Mantendo as fórmulas conforme as Equações 14 a 20, com Equação 16 corrigida)

def calc_derivative(x, y, idx, side):
    """
    Calcula a primeira derivada f'(x) em um ponto usando regras de
    diferenças finitas "constrained" conforme o lado do ponto.

    Parâmetros:
      - x: lista dos pontos do eixo independente.
      - y: lista dos pontos do eixo dependente.
      - idx: índice onde calcular a derivada.
      - side: 'left' para o primeiro ponto, 'right' para o último,
              ou 'interior' para os demais.
    """
    n = len(x)
    if side == 'left':
        if n > 2:
            cond = ((x[2]-x[1])/(y[2]-y[1])) - ((x[1]-x[0])/(y[1]-y[0]))
            if cond < 0:
                return (y[1]-y[0])/(x[1]-x[0])
            else:
                fprime_next = (y[2]-y[1])/(x[2]-x[1])
                return (3*(y[1]-y[0])/(2*(x[1]-x[0]))) - (fprime_next/2)
        else:
            return (y[1]-y[0])/(x[1]-x[0])
    elif side == 'right':
        if n > 2:
            cond = ((x[n-1]-x[n-2])/(y[n-1]-y[n-2])) - ((x[n-2]-x[n-3])/(y[n-2]-y[n-3]))
            if cond < 0:
                return (y[n-1]-y[n-2])/(x[n-1]-x[n-2])
            else:
                fprime_temp = (y[n-1]-y[n-2])/(x[n-1]-x[n-2])
                return (3*(y[n-1]-y[n-2])/(2*(x[n-1]-x[n-2]))) - (fprime_temp/2)
        else:
            return (y[n-1]-y[n-2])/(x[n-1]-x[n-2])
    else:
        if idx+1 < n-1:
            cond = ((x[idx+2]-x[idx+1])/(y[idx+2]-y[idx+1])) - ((x[idx+1]-x[idx])/(y[idx+1]-y[idx]))
            if cond < 0:
                return (y[idx+1]-y[idx-1])/(x[idx+1]-x[idx-1])
            else:
                fprime_next = (y[idx+2]-y[idx+1])/(x[idx+2]-x[idx+1])
                return (3*(y[idx+1]-y[idx])/(2*(x[idx+1]-x[idx]))) - (fprime_next/2)
        else:
            return (y[idx]-y[idx-1])/(x[idx]-x[idx-1])

def cubic_spline_interp(x, y, x_interp):
    """
    Realiza a interpolação cúbica "constrained" unidimensional usando as
    Equações (14) a (20). Se houver apenas 2 pontos, utiliza interpolação linear.
    """
    n = len(x)
    if n == 2:
        if x_interp <= x[0]:
            return y[0]
        if x_interp >= x[1]:
            return y[1]
        return y[0] + (y[1]-y[0])*((x_interp-x[0])/(x[1]-x[0]))
    
    if x_interp <= x[0]:
        return y[0]
    if x_interp >= x[-1]:
        return y[-1]
    i = 1
    while i < n and x_interp > x[i]:
        i += 1
    x0, x1 = x[i-1], x[i]
    y0, y1 = y[i-1], y[i]
    m0 = calc_derivative(x, y, 0 if i-1==0 else i-1, 'left' if i-1==0 else 'interior')
    m1 = calc_derivative(x, y, n-1 if i==n-1 else i, 'right' if i==n-1 else 'interior')
    f2_x0 = (2*(m1-m0))/(x1-x0) + 6*(y1-y0)/((x1-x0)**2)
    f2_x1 = (2*(m1+m0))/(x1-x0) + 6*(y1-y0)/((x1-x0)**2)
    d = (f2_x1 - f2_x0) / (6*(x1-x0))
    c = (x1*f2_x0 - x0*f2_x1) / (2*(x1-x0))
    b = ((y1-y0) - c*(x1**2-x0**2) - d*(x1**3-x0**3)) / (x1-x0)
    a = y0 - b*x0 - c*(x0**2) - d*(x0**3)
    return a + b*x_interp + c*(x_interp**2) + d*(x_interp**3)

def bidimensional_interp(deltas, tempos, vol_matrix, delta_interp, tempo_interp):
    """
    Realiza a interpolação bidimensional: primeiro interpola no eixo delta,
    depois no eixo tempo.
    """
    interp_por_tempo = [cubic_spline_interp(deltas, row, delta_interp) for row in vol_matrix]
    return cubic_spline_interp(tempos, interp_por_tempo, tempo_interp)

# --- Função Auxiliar para Processar uma Única Linha de Dados ---
def compute_multi_interp(volatilidades, tempos, deltas, query_deltas, query_tempos, ativo, data_referencia):
    """
    Processa uma linha de entrada (com arrays de dados medidos e de consulta)
    e retorna uma lista de dicionários, cada um representando uma linha de saída com:
      - Data_referencia
      - chave (formada por ativo, delta e tempo de consulta)
      - Volatilidade_Interpolada
    Se houver mais de uma medição para o mesmo par (tempo, delta), soma os valores.
    """
    data_dict = {}
    for i in range(len(volatilidades)):
        key = (tempos[i], deltas[i])
        data_dict[key] = data_dict.get(key, 0.0) + volatilidades[i]
    
    unique_tempos = sorted({t for t in tempos})
    unique_deltas = sorted({d for d in deltas})
    vol_matrix = [[data_dict.get((t, d), 0.0) for d in unique_deltas] for t in unique_tempos]
    
    result_list = []
    for qt in query_tempos:
        for qd in query_deltas:
            interp_val = bidimensional_interp(unique_deltas, unique_tempos, vol_matrix, qd, qt)
            # Formata a chave: delta formatado com zfill(3) (delta*10) e tempo com zfill(5)
            delta_str = str(int(round(qd * 10))).zfill(3)
            tempo_str = str(int(qt)).zfill(5)
            chave = f"{ativo}_{delta_str}_{tempo_str}"
            result_list.append({
                "Data_referencia": data_referencia,
                "chave": chave,
                "Volatilidade_Interpolada": interp_val
            })
    return result_list

# --- UDTF com pandas_udtf ---
# Essa UDTF recebe, para cada registro, os arrays de dados e de consulta e já retorna um DataFrame com as linhas formatadas.
output_schema = StructType([
    StructField("Data_referencia", StringType(), False),
    StructField("chave", StringType(), False),
    StructField("Volatilidade_Interpolada", DoubleType(), False)
])

@pandas_udtf(output_schema, functionType="iterator")
def multi_interp_udtf(volatilidades: pd.Series,
                      tempos: pd.Series,
                      deltas: pd.Series,
                      query_deltas: pd.Series,
                      query_tempos: pd.Series,
                      ativo: pd.Series,
                      data_referencia: pd.Series) -> Iterator[pd.DataFrame]:
    """
    UDTF que realiza a interpolação bidimensional da superfície de volatilidade.
    Para cada registro de entrada (com arrays de volatilidades, tempos, deltas,
    query_deltas e query_tempos, além do ativo e data_referencia), retorna um DataFrame
    com as colunas:
      - Data_referencia
      - chave (formato "ativo_{delta_formatado}_{tempo_formatado}")
      - Volatilidade_Interpolada
    """
    for i in range(len(volatilidades)):
        # Cada entrada é uma lista; assumimos que os valores já estão formatados como Python lists
        vol = volatilidades.iloc[i]
        t = tempos.iloc[i]
        d = deltas.iloc[i]
        qd = query_deltas.iloc[i]
        qt = query_tempos.iloc[i]
        a = ativo.iloc[i]
        dr = data_referencia.iloc[i]
        result_list = compute_multi_interp(vol, t, d, qd, qt, a, dr)
        yield pd.DataFrame(result_list)

# --- Exemplo de Aplicação no DataFrame de Consulta ---
# Suponha que o DataFrame de consulta contenha as colunas:
#   - Ativo
#   - Data_referencia
#   - volatilidades: array de floats (dados medidos)
#   - tempos: array de floats (dados medidos)
#   - deltas: array de floats (dados medidos)
#   - query_deltas: array de floats com os deltas de consulta
#   - query_tempos: array de floats com os tempos de consulta
data = [
    (
        "ABC",                     # Ativo
        "2025-03-31",              # Data_referencia
        [0.3, 0.5, 0.6, 0.1],        # volatilidades: ex. índice 0 -> (tempo=1.0, delta=0.05), 1 -> (tempo=2.0, delta=0.05), etc.
        [1.0, 2.0, 1.0, 2.0],        # tempos
        [0.05, 0.05, 0.1, 0.1],       # deltas
        [0.075, 0.085],             # query_deltas: somente os valores a consultar
        [1.5, 2.5]                  # query_tempos: somente os tempos a consultar
    )
]
columns = ["Ativo", "Data_referencia", "volatilidades", "tempos", "deltas", "query_deltas", "query_tempos"]
df = spark.createDataFrame(data, schema=columns)

# Aplica a UDTF; ela já retorna um DataFrame com as linhas formatadas conforme desejado.
final_df = df.selectExpr("multi_interp_udtf(volatilidades, tempos, deltas, query_deltas, query_tempos, Ativo, Data_referencia) as results") \
              .select("results.*")

final_df.show(truncate=False)
