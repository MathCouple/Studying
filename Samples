from pyspark.sql.functions import pandas_udf, col
import pandas as pd
from pyspark.sql.types import StructType, StructField, StringType, DoubleType
from bisect import bisect_left
from collections import defaultdict

# ====================================================================
# Função: calc_derivative
# ====================================================================
def calc_derivative(x, y, idx, side):
    """
    Calcula a primeira derivada f'(x) em um ponto usando regras de 
    diferenças finitas "constrained" segundo o lado do ponto (início, 
    interior ou final).

    Parâmetros:
      - x: lista dos pontos do eixo independente.
      - y: lista dos pontos do eixo dependente.
      - idx: índice do ponto onde a derivada será calculada.
      - side: 'left' para o primeiro ponto, 'right' para o último, 
              ou 'interior' para os demais.

    Regras aplicadas:
      1. Para o primeiro ponto (side == 'left'):
         - Se houver pelo menos 3 pontos, calcula:
           cond = ((x[2]-x[1])/(y[2]-y[1])) - ((x[1]-x[0])/(y[1]-y[0])).
         - Se cond < 0, utiliza a diferença simples:
           f'(x₀) = (y₁ - y₀)/(x₁ - x₀).
         - Caso contrário, utiliza a fórmula:
           f'(x₀) = [3(y₁ - y₀)]⁄[2(x₁ - x₀)] – f'(x₂)/2.
      
      2. Para o último ponto (side == 'right'):
         - Utiliza os últimos três pontos e aplica lógica similar ao caso inicial.
      
      3. Para pontos internos (side == 'interior'):
         - Se houver um ponto à frente (idx+2 < len(x)):
           - Calcula:
             cond = ((x[idx+2]-x[idx+1])/(y[idx+2]-y[idx+1])) - ((x[idx+1]-x[idx])/(y[idx+1]-y[idx])).
           - Se cond < 0, utiliza a diferença centrada:
             f'(xᵢ) = (y[i+1] - y[i-1])/(x[i+1] - x[i-1]).
           - Caso contrário, aplica:
             f'(xᵢ) = [3(y[i+1] - y[i])]/[2(x[i+1] - x[i])] – f'(x[i+2])/2.
         - Se não houver ponto à frente suficiente, usa a diferença simples.
    """
    n = len(x)
    if side == 'left':
        if n > 2:
            cond = ((x[2] - x[1]) / (y[2] - y[1])) - ((x[1] - x[0]) / (y[1] - y[0]))
            if cond < 0:
                return (y[1] - y[0]) / (x[1] - x[0])
            else:
                fprime_next = (y[2] - y[1]) / (x[2] - x[1])
                return (3 * (y[1] - y[0]) / (2 * (x[1] - x[0]))) - (fprime_next / 2)
        else:
            return (y[1] - y[0]) / (x[1] - x[0])
    elif side == 'right':
        if n > 2:
            cond = ((x[n-1] - x[n-2]) / (y[n-1] - y[n-2])) - ((x[n-2] - x[n-3]) / (y[n-2] - y[n-3]))
            if cond < 0:
                return (y[n-1] - y[n-2]) / (x[n-1] - x[n-2])
            else:
                fprime_temp = (y[n-1] - y[n-2]) / (x[n-1] - x[n-2])
                return (3 * (y[n-1] - y[n-2]) / (2 * (x[n-1] - x[n-2]))) - (fprime_temp / 2)
        else:
            return (y[n-1] - y[n-2]) / (x[n-1] - x[n-2])
    else:
        if idx + 1 < n - 1:
            cond = ((x[idx+2] - x[idx+1]) / (y[idx+2] - y[idx+1])) - ((x[idx+1] - x[idx]) / (y[idx+1] - y[idx]))
            if cond < 0:
                return (y[idx+1] - y[idx-1]) / (x[idx+1] - x[idx-1])
            else:
                fprime_next = (y[idx+2] - y[idx+1]) / (x[idx+2] - x[idx+1])
                return (3 * (y[idx+1] - y[idx]) / (2 * (x[idx+1] - x[idx]))) - (fprime_next / 2)
        else:
            return (y[idx] - y[idx-1]) / (x[idx] - x[idx-1])

# ====================================================================
# Função: bidimensional_interp
# ====================================================================
def bidimensional_interp(deltas, tempos, vol_matrix, delta_interp, tempo_interp):
    """ Realiza interpolação bidimensional em dois eixos (delta e tempo). """
    interp_por_tempo = [cubic_spline_interp(deltas, row, delta_interp) for row in vol_matrix]
    return cubic_spline_interp(tempos, interp_por_tempo, tempo_interp)

# ====================================================================
# UDF: multi_interp_udf
# ====================================================================
output_schema = StructType([
    StructField("Data_referencia", StringType(), False),
    StructField("chave", StringType(), False),
    StructField("Volatilidade_Interpolada", DoubleType(), False)
])

@pandas_udf(output_schema)
def multi_interp_udf(volatilidades: pd.Series,
                      tempos: pd.Series,
                      deltas: pd.Series,
                      query_deltas: pd.Series,
                      query_tempos: pd.Series,
                      ativo: pd.Series,
                      data_referencia: pd.Series) -> pd.DataFrame:
    """ UDF Pandas para Spark que realiza interpolação bidimensional. """
    results = []
    for vol, t, d, qd, qt, a, dr in zip(volatilidades, tempos, deltas, query_deltas, query_tempos, ativo, data_referencia):
        data_dict = defaultdict(float)
        for ti, di, vi in zip(t, d, vol):
            data_dict[(ti, di)] += vi

        unique_tempos = sorted(set(t))
        unique_deltas = sorted(set(d))
        vol_matrix = [[data_dict[(ti, di)] for di in unique_deltas] for ti in unique_tempos]

        for qti in qt:
            for qdi in qd:
                chave = f"{a}_{str(int(round(qdi * 10))).zfill(3)}_{str(int(qti)).zfill(5)}"
                interp_val = bidimensional_interp(unique_deltas, unique_tempos, vol_matrix, qdi, qti)
                results.append((dr, chave, interp_val))
    
    return pd.DataFrame(results, columns=["Data_referencia", "chave", "Volatilidade_Interpolada"])

# ====================================================================
# Aplicação no DataFrame
# ====================================================================
final_df = df.withColumn("results", multi_interp_udf(
    col("volatilidades"), col("tempos"), col("deltas"),
    col("query_deltas"), col("query_tempos"), col("Ativo"), col("Data_referencia")
)).select("results.*")

final_df.show(truncate=False)
