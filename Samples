from pyspark.sql.functions import udf, explode, col
from pyspark.sql.types import DoubleType, StringType, ArrayType, StructType, StructField
import pyspark.sql.functions as F

# Função auxiliar para extrair valores únicos preservando a ordem de aparição
def unique_preserve_order(seq):
    seen = set()
    unique = []
    for item in seq:
        if item not in seen:
            unique.append(item)
            seen.add(item)
    return unique

# ====================================================================
# Função: calc_derivative
# ====================================================================
def calc_derivative(x, y, idx, side):
    """
    Calcula a primeira derivada f'(x) em um ponto usando regras de 
    diferenças finitas "constrained" segundo o lado do ponto (início, 
    interior ou final).

    Parâmetros:
      - x: lista dos pontos do eixo independente.
      - y: lista dos pontos do eixo dependente.
      - idx: índice do ponto onde a derivada será calculada.
      - side: 'left' para o primeiro, 'right' para o último ou 'interior'.
    
    Regras:
      - Para o primeiro ponto, se houver pelo menos 3 pontos, calcula:
          cond = ((x[2]-x[1])/(y[2]-y[1])) - ((x[1]-x[0])/(y[1]-y[0])).
        Se cond < 0, usa a diferença simples; caso contrário, aplica a fórmula ajustada.
      - Para o último ponto, aplica lógica similar com os três últimos pontos.
      - Para pontos internos, se houver um ponto à frente, calcula cond e usa a diferença centrada ou fórmula ajustada; senão, usa diferença simples.
    """
    n = len(x)
    if side == 'left':
        if n > 2:
            cond = ((x[2] - x[1]) / (y[2] - y[1])) - ((x[1] - x[0]) / (y[1] - y[0]))
            if cond < 0:
                return (y[1] - y[0]) / (x[1] - x[0])
            else:
                fprime_next = (y[2] - y[1]) / (x[2] - x[1])
                return (3 * (y[1] - y[0]) / (2 * (x[1] - x[0]))) - (fprime_next / 2)
        else:
            return (y[1] - y[0]) / (x[1] - x[0])
    elif side == 'right':
        if n > 2:
            cond = ((x[n-1] - x[n-2]) / (y[n-1] - y[n-2])) - ((x[n-2] - x[n-3]) / (y[n-2] - y[n-3]))
            if cond < 0:
                return (y[n-1] - y[n-2]) / (x[n-1] - x[n-2])
            else:
                fprime_temp = (y[n-1] - y[n-2]) / (x[n-1] - x[n-2])
                return (3 * (y[n-1] - y[n-2]) / (2 * (x[n-1] - x[n-2]))) - (fprime_temp / 2)
        else:
            return (y[n-1] - y[n-2]) / (x[n-1] - x[n-2])
    else:
        if idx + 1 < n - 1:
            cond = ((x[idx+2] - x[idx+1]) / (y[idx+2] - y[idx+1])) - ((x[idx+1] - x[idx]) / (y[idx+1] - y[idx]))
            if cond < 0:
                return (y[idx+1] - y[idx-1]) / (x[idx+1] - x[idx-1])
            else:
                fprime_next = (y[idx+2] - y[idx+1]) / (x[idx+2] - x[idx+1])
                return (3 * (y[idx+1] - y[idx]) / (2 * (x[idx+1] - x[idx]))) - (fprime_next / 2)
        else:
            return (y[idx] - y[idx-1]) / (x[idx] - x[idx-1])

# ====================================================================
# Função: cubic_spline_interp
# ====================================================================
def cubic_spline_interp(x, y, x_interp):
    """
    Realiza a interpolação cúbica "constrained" unidimensional usando as 
    Equações (14) a (20).

    Passos:
      1. Se x_interp estiver fora dos limites de x, retorna y na extremidade.
      2. Encontra o intervalo [x[i-1], x[i]] onde x_interp se localiza.
      3. Calcula as derivadas m₀ e m₁ usando calc_derivative.
      4. Calcula as segundas derivadas:
           f2_x0 = [2(m₁ - m₀)]/(x₁-x₀) + 6*(y₁-y₀)/((x₁-x₀)²)
           f2_x1 = [2(m₁ + m₀)]/(x₁-x₀) + 6*(y₁-y₀)/((x₁-x₀)²)
      5. Calcula os coeficientes:
           d = (f2_x1 - f2_x0) / (6*(x₁-x₀))
           c = (x₁*f2_x0 - x₀*f2_x1) / (2*(x₁-x₀))
           b = ((y₁-y₀) - c*(x₁²-x₀²) - d*(x₁³-x₀³)) / (x₁-x₀)
           a = y₀ - b*x₀ - c*x₀² - d*x₀³
      6. Retorna y(x_interp) = a + b*x_interp + c*x_interp² + d*x_interp³.
      
      Se houver apenas 2 pontos, usa interpolação linear.
    """
    n = len(x)
    if n == 2:
        if x_interp <= x[0]:
            return y[0]
        if x_interp >= x[1]:
            return y[1]
        return y[0] + (y[1] - y[0]) * ((x_interp - x[0]) / (x[1] - x[0]))
    
    if x_interp <= x[0]:
        return y[0]
    if x_interp >= x[-1]:
        return y[-1]
    i = 1
    while i < n and x_interp > x[i]:
        i += 1
    x0, x1 = x[i-1], x[i]
    y0, y1 = y[i-1], y[i]
    m0 = calc_derivative(x, y, 0 if i-1 == 0 else i-1, 'left' if i-1 == 0 else 'interior')
    m1 = calc_derivative(x, y, n-1 if i == n-1 else i, 'right' if i == n-1 else 'interior')
    f2_x0 = (2 * (m1 - m0)) / (x1 - x0) + (6 * (y1 - y0)) / ((x1 - x0)**2)
    f2_x1 = (2 * (m1 + m0)) / (x1 - x0) + (6 * (y1 - y0)) / ((x1 - x0)**2)
    d = (f2_x1 - f2_x0) / (6 * (x1 - x0))
    c = (x1 * f2_x0 - x0 * f2_x1) / (2 * (x1 - x0))
    b = ((y1 - y0) - c * (x1**2 - x0**2) - d * (x1**3 - x0**3)) / (x1 - x0)
    a = y0 - b * x0 - c * (x0**2) - d * (x0**3)
    return a + b * x_interp + c * (x_interp**2) + d * (x_interp**3)

# ====================================================================
# Função: bidimensional_interp
# ====================================================================
def bidimensional_interp(deltas, tempos, vol_matrix, delta_interp, tempo_interp):
    """
    Realiza a interpolação bidimensional usando:
      1. Interpolação no eixo delta para cada linha (tempo) da matriz.
      2. Interpolação no eixo tempo utilizando os resultados do passo 1.
    """
    interp_por_tempo = [cubic_spline_interp(deltas, row, delta_interp) for row in vol_matrix]
    return cubic_spline_interp(tempos, interp_por_tempo, tempo_interp)

# ====================================================================
# UDF: multi_interp_udf
# ====================================================================
# Schema do resultado: array de structs com "chave" (string) e "volatilidade_interpolada" (double)
schema_result = ArrayType(StructType([
    StructField("chave", StringType(), False),
    StructField("volatilidade_interpolada", DoubleType(), False)
]))

@udf(schema_result)
def multi_interp_udf(volatilidades, tempos, deltas, query_deltas, query_tempos, ativo, data_referencia):
    """
    UDF para Spark que realiza a interpolação bidimensional da superfície de volatilidade.

    Parâmetros:
      - volatilidades: array linear de floats (tamanho N) com os dados medidos (cada índice é um ponto: (tempo, delta)).
      - tempos: array de floats com os tempos medidos (a ordem é a mesma dos índices).
      - deltas: array de floats com os deltas medidos (a ordem é a mesma dos índices).
      - query_deltas: array de floats com os deltas de consulta.
      - query_tempos: array de floats com os tempos de consulta.
      - ativo: string identificadora do ativo.
      - data_referencia: data (ou string) de referência.
    
    Processo:
      1. Monta um dicionário mapeando (tempo, delta) para a soma das volatilidades, ignorando entradas vazias.
      2. Extrai os eixos medidos na ordem de aparição usando unique_preserve_order.
      3. Reconstrói a matriz 2D de volatilidades a partir do dicionário.
      4. Itera apenas sobre os valores passados em query_deltas e query_tempos para gerar exatamente DxT saídas.
      5. Para cada ponto de consulta, formata:
           - O delta: multiplica por 1000, arredonda e preenche com zeros à esquerda (3 dígitos).
           - O tempo: converte para inteiro e preenche com zeros à esquerda (5 dígitos).
         A chave é formada no formato "delta_formatado_venc_formatado".
      6. Retorna um array de structs com a chave e a volatilidade interpolada.
    """
    data_dict = {}
    for i in range(len(volatilidades)):
        # Ignora registros com valores vazios em tempos ou deltas
        if tempos[i] is None or deltas[i] is None or str(tempos[i]).strip() == "" or str(deltas[i]).strip() == "":
            continue
        key = (tempos[i], deltas[i])
        data_dict[key] = data_dict.get(key, 0.0) + volatilidades[i]
    
    # Extrai os eixos medidos preservando a ordem de aparição
    unique_tempos = unique_preserve_order(tempos)
    unique_deltas = unique_preserve_order(deltas)
    
    # Reconstrói a matriz 2D; para cada tempo na ordem, para cada delta na ordem
    vol_matrix = [[data_dict.get((t, d), 0.0) for d in unique_deltas] for t in unique_tempos]
    
    # Itera sobre as consultas exatamente como passadas (sem ordenar os query)
    results = [
        {
            # Formata o delta: multiplica por 1000, arredonda e preenche com zeros à esquerda (3 dígitos)
            "chave": f"{str(int(round(qd*1000))).zfill(3)}_{str(int(round(qt))).zfill(5)}",
            "volatilidade_interpolada": bidimensional_interp(unique_deltas, unique_tempos, vol_matrix, qd, qt)
        }
        for qt in query_tempos
        for qd in query_deltas
    ]
    return results

# ====================================================================
# Exemplo de Aplicação no DataFrame de Consulta
# ====================================================================
data = [
    (
        "ABC",                     # Ativo
        "2025-03-31",              # Data_referencia
        [0.3, 0.5, 0.6, 0.1],        # volatilidades: ex. índice 0 -> (tempo=1.0, delta=0.05), índice 1 -> (tempo=2.0, delta=0.05), etc.
        [1.0, 2.0, 1.0, 2.0],        # tempos (a ordem reflete as três entradas concomitantes)
        [0.05, 0.05, 0.1, 0.1],       # deltas (a ordem é a mesma)
        [0.075, 0.085],             # query_deltas (exatamente os deltas desejados para consulta)
        [1.5, 2.5]                  # query_tempos (exatamente os tempos desejados para consulta)
    )
]
columns = ["Ativo", "Data_referencia", "volatilidades", "tempos", "deltas", "query_deltas", "query_tempos"]
df = spark.createDataFrame(data, schema=columns)

df = df.withColumn("results", multi_interp_udf("volatilidades", "tempos", "deltas", "query_deltas", "query_tempos", "Ativo", "Data_referencia"))
final_df = df.select("Data_referencia", "Ativo", F.explode("results").alias("result")) \
              .select("Data_referencia", "Ativo", col("result.chave").alias("chave"), col("result.volatilidade_interpolada").alias("Volatilidade_Interpolada"))

final_df.show(truncate=False)from pyspark.sql.functions import udf, explode, col
from pyspark.sql.types import DoubleType, StringType, ArrayType, StructType, StructField
import pyspark.sql.functions as F

# Função auxiliar para extrair valores únicos preservando a ordem de aparição
def unique_preserve_order(seq):
    seen = set()
    unique = []
    for item in seq:
        if item not in seen:
            unique.append(item)
            seen.add(item)
    return unique

# ====================================================================
# Função: calc_derivative
# ====================================================================
def calc_derivative(x, y, idx, side):
    """
    Calcula a primeira derivada f'(x) em um ponto usando regras de 
    diferenças finitas "constrained" segundo o lado do ponto (início, 
    interior ou final).

    Parâmetros:
      - x: lista dos pontos do eixo independente.
      - y: lista dos pontos do eixo dependente.
      - idx: índice do ponto onde a derivada será calculada.
      - side: 'left' para o primeiro, 'right' para o último ou 'interior'.
    
    Regras:
      - Para o primeiro ponto, se houver pelo menos 3 pontos, calcula:
          cond = ((x[2]-x[1])/(y[2]-y[1])) - ((x[1]-x[0])/(y[1]-y[0])).
        Se cond < 0, usa a diferença simples; caso contrário, aplica a fórmula ajustada.
      - Para o último ponto, aplica lógica similar com os três últimos pontos.
      - Para pontos internos, se houver um ponto à frente, calcula cond e usa a diferença centrada ou fórmula ajustada; senão, usa diferença simples.
    """
    n = len(x)
    if side == 'left':
        if n > 2:
            cond = ((x[2] - x[1]) / (y[2] - y[1])) - ((x[1] - x[0]) / (y[1] - y[0]))
            if cond < 0:
                return (y[1] - y[0]) / (x[1] - x[0])
            else:
                fprime_next = (y[2] - y[1]) / (x[2] - x[1])
                return (3 * (y[1] - y[0]) / (2 * (x[1] - x[0]))) - (fprime_next / 2)
        else:
            return (y[1] - y[0]) / (x[1] - x[0])
    elif side == 'right':
        if n > 2:
            cond = ((x[n-1] - x[n-2]) / (y[n-1] - y[n-2])) - ((x[n-2] - x[n-3]) / (y[n-2] - y[n-3]))
            if cond < 0:
                return (y[n-1] - y[n-2]) / (x[n-1] - x[n-2])
            else:
                fprime_temp = (y[n-1] - y[n-2]) / (x[n-1] - x[n-2])
                return (3 * (y[n-1] - y[n-2]) / (2 * (x[n-1] - x[n-2]))) - (fprime_temp / 2)
        else:
            return (y[n-1] - y[n-2]) / (x[n-1] - x[n-2])
    else:
        if idx + 1 < n - 1:
            cond = ((x[idx+2] - x[idx+1]) / (y[idx+2] - y[idx+1])) - ((x[idx+1] - x[idx]) / (y[idx+1] - y[idx]))
            if cond < 0:
                return (y[idx+1] - y[idx-1]) / (x[idx+1] - x[idx-1])
            else:
                fprime_next = (y[idx+2] - y[idx+1]) / (x[idx+2] - x[idx+1])
                return (3 * (y[idx+1] - y[idx]) / (2 * (x[idx+1] - x[idx]))) - (fprime_next / 2)
        else:
            return (y[idx] - y[idx-1]) / (x[idx] - x[idx-1])

# ====================================================================
# Função: cubic_spline_interp
# ====================================================================
def cubic_spline_interp(x, y, x_interp):
    """
    Realiza a interpolação cúbica "constrained" unidimensional usando as 
    Equações (14) a (20).

    Passos:
      1. Se x_interp estiver fora dos limites de x, retorna y na extremidade.
      2. Encontra o intervalo [x[i-1], x[i]] onde x_interp se localiza.
      3. Calcula as derivadas m₀ e m₁ usando calc_derivative.
      4. Calcula as segundas derivadas:
           f2_x0 = [2(m₁ - m₀)]/(x₁-x₀) + 6*(y₁-y₀)/((x₁-x₀)²)
           f2_x1 = [2(m₁ + m₀)]/(x₁-x₀) + 6*(y₁-y₀)/((x₁-x₀)²)
      5. Calcula os coeficientes:
           d = (f2_x1 - f2_x0) / (6*(x₁-x₀))
           c = (x₁*f2_x0 - x₀*f2_x1) / (2*(x₁-x₀))
           b = ((y₁-y₀) - c*(x₁²-x₀²) - d*(x₁³-x₀³)) / (x₁-x₀)
           a = y₀ - b*x₀ - c*x₀² - d*x₀³
      6. Retorna y(x_interp) = a + b*x_interp + c*x_interp² + d*x_interp³.
      
      Se houver apenas 2 pontos, usa interpolação linear.
    """
    n = len(x)
    if n == 2:
        if x_interp <= x[0]:
            return y[0]
        if x_interp >= x[1]:
            return y[1]
        return y[0] + (y[1] - y[0]) * ((x_interp - x[0]) / (x[1] - x[0]))
    
    if x_interp <= x[0]:
        return y[0]
    if x_interp >= x[-1]:
        return y[-1]
    i = 1
    while i < n and x_interp > x[i]:
        i += 1
    x0, x1 = x[i-1], x[i]
    y0, y1 = y[i-1], y[i]
    m0 = calc_derivative(x, y, 0 if i-1 == 0 else i-1, 'left' if i-1 == 0 else 'interior')
    m1 = calc_derivative(x, y, n-1 if i == n-1 else i, 'right' if i == n-1 else 'interior')
    f2_x0 = (2 * (m1 - m0)) / (x1 - x0) + (6 * (y1 - y0)) / ((x1 - x0)**2)
    f2_x1 = (2 * (m1 + m0)) / (x1 - x0) + (6 * (y1 - y0)) / ((x1 - x0)**2)
    d = (f2_x1 - f2_x0) / (6 * (x1 - x0))
    c = (x1 * f2_x0 - x0 * f2_x1) / (2 * (x1 - x0))
    b = ((y1 - y0) - c * (x1**2 - x0**2) - d * (x1**3 - x0**3)) / (x1 - x0)
    a = y0 - b * x0 - c * (x0**2) - d * (x0**3)
    return a + b * x_interp + c * (x_interp**2) + d * (x_interp**3)

# ====================================================================
# Função: bidimensional_interp
# ====================================================================
def bidimensional_interp(deltas, tempos, vol_matrix, delta_interp, tempo_interp):
    """
    Realiza a interpolação bidimensional usando:
      1. Interpolação no eixo delta para cada linha (tempo) da matriz.
      2. Interpolação no eixo tempo utilizando os resultados do passo 1.
    """
    interp_por_tempo = [cubic_spline_interp(deltas, row, delta_interp) for row in vol_matrix]
    return cubic_spline_interp(tempos, interp_por_tempo, tempo_interp)

# ====================================================================
# UDF: multi_interp_udf
# ====================================================================
# Schema do resultado: array de structs com "chave" (string) e "volatilidade_interpolada" (double)
schema_result = ArrayType(StructType([
    StructField("chave", StringType(), False),
    StructField("volatilidade_interpolada", DoubleType(), False)
]))

@udf(schema_result)
def multi_interp_udf(volatilidades, tempos, deltas, query_deltas, query_tempos, ativo, data_referencia):
    """
    UDF para Spark que realiza a interpolação bidimensional da superfície de volatilidade.

    Parâmetros:
      - volatilidades: array linear de floats (tamanho N) com os dados medidos (cada índice é um ponto: (tempo, delta)).
      - tempos: array de floats com os tempos medidos (a ordem é a mesma dos índices).
      - deltas: array de floats com os deltas medidos (a ordem é a mesma dos índices).
      - query_deltas: array de floats com os deltas de consulta.
      - query_tempos: array de floats com os tempos de consulta.
      - ativo: string identificadora do ativo.
      - data_referencia: data (ou string) de referência.
    
    Processo:
      1. Monta um dicionário mapeando (tempo, delta) para a soma das volatilidades, ignorando entradas vazias.
      2. Extrai os eixos medidos na ordem de aparição usando unique_preserve_order.
      3. Reconstrói a matriz 2D de volatilidades a partir do dicionário.
      4. Itera apenas sobre os valores passados em query_deltas e query_tempos para gerar exatamente DxT saídas.
      5. Para cada ponto de consulta, formata:
           - O delta: multiplica por 1000, arredonda e preenche com zeros à esquerda (3 dígitos).
           - O tempo: converte para inteiro e preenche com zeros à esquerda (5 dígitos).
         A chave é formada no formato "delta_formatado_venc_formatado".
      6. Retorna um array de structs com a chave e a volatilidade interpolada.
    """
    data_dict = {}
    for i in range(len(volatilidades)):
        # Ignora registros com valores vazios em tempos ou deltas
        if tempos[i] is None or deltas[i] is None or str(tempos[i]).strip() == "" or str(deltas[i]).strip() == "":
            continue
        key = (tempos[i], deltas[i])
        data_dict[key] = data_dict.get(key, 0.0) + volatilidades[i]
    
    # Extrai os eixos medidos preservando a ordem de aparição
    unique_tempos = unique_preserve_order(tempos)
    unique_deltas = unique_preserve_order(deltas)
    
    # Reconstrói a matriz 2D; para cada tempo na ordem, para cada delta na ordem
    vol_matrix = [[data_dict.get((t, d), 0.0) for d in unique_deltas] for t in unique_tempos]
    
    # Itera sobre as consultas exatamente como passadas (sem ordenar os query)
    results = [
        {
            # Formata o delta: multiplica por 1000, arredonda e preenche com zeros à esquerda (3 dígitos)
            "chave": f"{str(int(round(qd*1000))).zfill(3)}_{str(int(round(qt))).zfill(5)}",
            "volatilidade_interpolada": bidimensional_interp(unique_deltas, unique_tempos, vol_matrix, qd, qt)
        }
        for qt in query_tempos
        for qd in query_deltas
    ]
    return results

# ====================================================================
# Exemplo de Aplicação no DataFrame de Consulta
# ====================================================================
data = [
    (
        "ABC",                     # Ativo
        "2025-03-31",              # Data_referencia
        [0.3, 0.5, 0.6, 0.1],        # volatilidades: ex. índice 0 -> (tempo=1.0, delta=0.05), índice 1 -> (tempo=2.0, delta=0.05), etc.
        [1.0, 2.0, 1.0, 2.0],        # tempos (a ordem reflete as três entradas concomitantes)
        [0.05, 0.05, 0.1, 0.1],       # deltas (a ordem é a mesma)
        [0.075, 0.085],             # query_deltas (exatamente os deltas desejados para consulta)
        [1.5, 2.5]                  # query_tempos (exatamente os tempos desejados para consulta)
    )
]
columns = ["Ativo", "Data_referencia", "volatilidades", "tempos", "deltas", "query_deltas", "query_tempos"]
df = spark.createDataFrame(data, schema=columns)

df = df.withColumn("results", multi_interp_udf("volatilidades", "tempos", "deltas", "query_deltas", "query_tempos", "Ativo", "Data_referencia"))
final_df = df.select("Data_referencia", "Ativo", F.explode("results").alias("result")) \
              .select("Data_referencia", "Ativo", col("result.chave").alias("chave"), col("result.volatilidade_interpolada").alias("Volatilidade_Interpolada"))

final_df.show(truncate=False)
